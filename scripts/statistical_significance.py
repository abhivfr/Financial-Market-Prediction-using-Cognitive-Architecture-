# Save as scripts/statistical_significance.py
import numpy as np
import pandas as pd
from scipy import stats
# import argparse # Removed argparse
import json
import os
import torch # Import torch for tensor conversion
import sys # Import sys for exiting on error

# Assuming calculate_* functions are importable from your train.py
# You might need to adjust the import path if they are elsewhere
try:
    from train import calculate_price_accuracy, calculate_volume_correlation
    from train import calculate_returns_stability, calculate_volatility_prediction
    print("Successfully imported metric calculation functions.")
except ImportError as e:
    print(f"Error importing metric calculation functions: {e}")
    print("Please ensure calculate_price_accuracy, calculate_volume_correlation, calculate_returns_stability, and calculate_volatility_prediction are available and importable from your 'train' module or adjust the import path.")
    sys.exit(1) # Exit if imports fail


def bootstrap_significance(model_a_preds, model_b_preds, true_values, metric_func, n_bootstrap=1000, alpha=0.05):
    """
    Perform bootstrap significance testing between two models based on a metric.
    Assumes metric_func takes (predictions, true_values) and returns a scalar score.
    Higher scores are assumed better for determining 'better_model'.
    """
    n_samples = len(true_values)

    if n_samples == 0:
        print("Warning: No samples available for bootstrap testing.")
        return {
            'observed_diff': 0.0,
            'p_value': 1.0,
            'significant': False,
            'better_model': 'None'
        }

    # Calculate observed difference in metrics
    # Ensure metric_func returns a comparable number (use .item() if it returns a tensor)
    try:
        # Convert numpy arrays to torch tensors for metric function
        metric_a = metric_func(torch.tensor(model_a_preds), torch.tensor(true_values))
        metric_b = metric_func(torch.tensor(model_b_preds), torch.tensor(true_values))
        observed_diff = (metric_a - metric_b).item() if torch.is_tensor(metric_a - metric_b) else (metric_a - metric_b)

    except Exception as e:
        print(f"Error calculating observed metric difference: {e}")
        return {
            'observed_diff': 0.0,
            'p_value': 1.0,
            'significant': False,
            'better_model': 'Error'
        }


    # Bootstrap sampling
    bootstrap_diffs = []
    # Convert numpy arrays for easier indexing
    model_a_preds_np = np.asarray(model_a_preds)
    model_b_preds_np = np.asarray(model_b_preds)
    true_values_np = np.asarray(true_values)


    print(f"Running {n_bootstrap} bootstrap samples...")
    # Using tqdm for a progress bar is helpful here, but requires 'tqdm' package
    # from tqdm import tqdm
    # for i in tqdm(range(n_bootstrap), desc="Bootstrapping"):
    for i in range(n_bootstrap):
        # Sample indices with replacement
        indices = np.random.choice(n_samples, n_samples, replace=True)

        # Calculate metric difference for this sample using sampled indices
        try:
            sample_metric_a = metric_func(torch.tensor(model_a_preds_np[indices]), torch.tensor(true_values_np[indices]))
            sample_metric_b = metric_func(torch.tensor(model_b_preds_np[indices]), torch.tensor(true_values_np[indices]))
            sample_diff = (sample_metric_a - sample_metric_b).item() if torch.is_tensor(sample_metric_a - sample_metric_b) else (sample_metric_a - sample_metric_b)
            bootstrap_diffs.append(sample_diff)
        except Exception as e:
            # Handle potential errors during sampling (unlikely but robust)
            # print(f"Warning: Error during bootstrap sample {i}: {e}. Skipping sample.") # Suppress frequent warnings
            continue # Skip this sample if calculation fails

    if not bootstrap_diffs:
         print("Error: No successful bootstrap samples were computed.")
         return {
            'observed_diff': observed_diff,
            'p_value': 1.0, # Cannot determine significance
            'significant': False,
            'better_model': 'Undetermined'
        }

    bootstrap_diffs_np = np.array(bootstrap_diffs)

    # Calculate p-value (two-tailed test)
    if observed_diff >= 0:
        p_value = np.mean(bootstrap_diffs_np <= 0)
    else:
        p_value = np.mean(bootstrap_diffs_np >= 0)

    p_value = min(p_value * 2, 1.0)

    return {
        'observed_diff': observed_diff,
        'p_value': p_value,
        'significant': p_value < alpha,
        'better_model': 'A' if observed_diff > 0 else ('B' if observed_diff < 0 else 'Tie')
    }


# --- Main execution block with hardcoded values ---
if __name__ == "__main__":
    # Hardcoded values to bypass command-line arguments
    # Ensure these paths match the CSVs generated by analyze_performance.py
    predictions_a_path = "validation/results/cognitive_predictions.csv"
    predictions_b_path = "validation/results/baseline_predictions.csv"
    ground_truth_path = "validation/results/ground_truth.csv"
    output_path = "validation/significance_test.json"
    n_bootstrap_samples = 1000
    significance_alpha = 0.05

    print("Running statistical significance testing with hardcoded parameters...")


    # Load predictions and ground truth from CSVs
    try:
        # Read CSVs and convert to numpy arrays
        preds_a = pd.read_csv(predictions_a_path).values
        preds_b = pd.read_csv(predictions_b_path).values
        truth = pd.read_csv(ground_truth_path).values
        print(f"Loaded predictions from {predictions_a_path} ({preds_a.shape}) and {predictions_b_path} ({preds_b.shape})")
        print(f"Loaded ground truth from {ground_truth_path} ({truth.shape})")

        # Ensure shapes match for comparison
        if preds_a.shape != truth.shape or preds_b.shape != truth.shape:
             print("Error: Prediction and ground truth shapes do not match!")
             print(f"Shape A: {preds_a.shape}, Shape B: {preds_b.shape}, Truth Shape: {truth.shape}")
             # Optionally, truncate to the smallest number of samples if shapes mismatch but columns match
             min_samples = min(preds_a.shape[0], preds_b.shape[0], truth.shape[0])
             if preds_a.shape[1] == truth.shape[1] and preds_b.shape[1] == truth.shape[1]:
                  print(f"Warning: Truncating to {min_samples} samples for comparison due to shape mismatch.")
                  preds_a = preds_a[:min_samples]
                  preds_b = preds_b[:min_samples]
                  truth = truth[:min_samples]
             else:
                 print("Error: Column counts do not match. Cannot perform comparison.")
                 sys.exit(1) # Exit if shapes don't match


    except FileNotFoundError as e:
        print(f"Error: Input file not found: {e}")
        sys.exit(1) # Exit if files not found
    except Exception as e:
        print(f"Error loading data from CSVs: {e}")
        sys.exit(1) # Exit on other loading errors


    # Define metric functions
    # The functions imported from train are expected to take PyTorch tensors
    metric_funcs = {
        'price_accuracy': calculate_price_accuracy,
        'volume_correlation': calculate_volume_correlation,
        'returns_stability': calculate_returns_stability,
        'volatility_prediction': calculate_volatility_prediction
    }

    results = {}
    print("\n--- Running Statistical Significance Tests ---")
    for name, func in metric_funcs.items():
        print(f"\nTesting metric: {name}")
        # Pass pred_a (cognitive) as A, pred_b (baseline) as B
        # Based on your printout, Cognitive seems to be Model A and Baseline is Model B in your previous script.
        # Let's assume 'A' in bootstrap_significance corresponds to Cognitive and 'B' to Baseline.
        results[name] = bootstrap_significance(
            preds_a, preds_b, truth, func, # Cognitive vs Baseline
            n_bootstrap=n_bootstrap_samples, alpha=significance_alpha
        )

    # Save results
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    try:
        # Ensure boolean values are standard Python bool before saving (workaround for unusual JSON error)
        # This snippet is placed just before the json.dump call
        for metric, result in results.items():
             if 'significant' in result:
                 # Explicitly cast to standard bool type
                 result['significant'] = bool(result['significant'])

        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nSignificance test results saved to {output_path}")
    except Exception as e:
        print(f"Error saving significance test results to JSON: {e}")


    # Print summary
    print("\n--- Significance Test Results Summary ---")
    print("Metric            | Observed Diff (Cog - Base) | p-value | Significant? | Better Model") # Added clarification in header
    print("-------------------|----------------------------|---------|--------------|--------------")
    for metric, result in results.items():
        sig_star = "Yes" if result.get('significant', False) else "No"
        better_model = result.get('better_model', 'N/A')
        observed_diff = result.get('observed_diff', 0.0)
        p_value = result.get('p_value', 1.0)
        print(f"{metric:<18} | {observed_diff:<26.4f} | {p_value:<7.4f} | {sig_star:<12} | {better_model}")
    print("-------------------|----------------------------|---------|--------------|--------------")
